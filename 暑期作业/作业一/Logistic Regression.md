# Logistic Regression


## 线性回归
在讲Logistic Regression之前，我们需要先讲线性回归。线性回归作为一种线性模型，有着下面的形式：
有数据集$D = \{(x_1, y_1), (x_2, y_2),...,(x_m, y_m)\}$,其中$x_i = (x_{i1};x_{i2};...;x_{id})$, $y_i \in R$.然后有：
$$f(x_i) = w^Tx_i + b，使得f(x_i) \simeq y_i$$

我们将数据集放入模型之中，然后输出与预测值，然后将这些预测值与真实值进行比较，然后构造损失函数，计算起损失值一般我们采用均方误差来作为损失函数。损失函数有着下面的形式：
$$(w^*, b^*) = \mathop{\arg\min}_{(w,b)} \sum_{i=1}^m(f(x_i) - y_i)^2$$
然后我们可以将这均方误差进行最小化，然后求解其中的$w,b$.由于这个损失函数是一个凸函数，所以可以采用最小二乘法来进行求解，其局部最优解就是全局最优解。我们对$w,b$分别进行求导，然后令其等于零，这样就可以求得其$w,b$值。还可以采用另一种方法，就是梯度下降法，其形式，就是也对$w,b$进行求导，求得其梯度后，我们用用原来的值来减去梯度乘以学习率的积，然后一次次迭代，最后达到最优解。

## 分析
虽然线性回归可以用于分类，但是线性回归有一个缺陷，那就是，当某一类的数据过于正确，就是数据离分类界面过于远，在计算损失时会有很大的误差，就会导致其不能很好的对数据进行分类。在对多分类任务中，如果有三个类别，如1代表类别一，2代表类别二，3代表类别三，但是由于类别一和二相差只有1，但是这却是两个完全不同的关系。所以，线性回归不能作为分类器，所以我们引入$Sigmoid$函数，将线性回归的输出作为$Sigmoid$函数的输入，然后其输出值就是就是相应的类型。

## Logistic Regression
逻辑回归的形式如下所示：
\begin{align*}
y = \frac{1}{1 + e^{-(w^Tx + b)}}
\end{align*}
在这种形势下，我们对线性回归的输出值进行了一个限制，让其值只能出现在0和1之间，这样，在其值大于0.5的情况下属于类别1，在其值小于0.5的情况下属于类别0，这样就能对数据集进行分类了。而且$sigmoid$函数是可微的，所以可以采用梯度下降的方法求解。
将其变形后如下所示：
\begin{align*}
ln\frac{y}{1-y} = w^Tx + b
\end{align*}
其实$y$就是样本$x$作为正例的可能性，而$1-y$则是其反例的可能性，这样样例的“几率”$\frac{y}{1-y}$反应了作为正例的相对可能性，对其取对数之后，就得到“对数几率”，即$ln\frac{y}{1-y}$,其实这就是在用线性回归模型的预测值来去逼近真实标记的对数几率。所以我们在求其逻辑回归模型的值时，其实就是在求其的真实概率。如下所示：
\begin{align*}
p(y=1|x)= \frac{e^{w^Tx + b}}{1 + e^{w^Tx + b}} \\
p(y=0|x)= \frac{1}{1 + e^{w^Tx + b}} 
\end{align*}
然后我们可以最大似然法来对其进行求解：
$$\zeta(w,b) = \sum_{i=1}^{m} ln p(y_i|x_i;w,b)$$
我们希望每个样本属于自己的真实标记的概率越大越好，所以我们就要求得在上面公式最大值的情况下的$w, b$值。我们对似然公式进行变形，如下所示：
$$\zeta(w,b) = \sum_{i=1}^{m}y_ilnp_1 + (1 - y_i)ln(1- p_1)$$
我们于是就需要将上面的式子最大化，加上负号后就是最小化，即：
$$-\zeta(w,b) = -(\sum_{i=1}^{m}y_ilnp_1 + (1 - y_i)ln(1-p_1))$$
在这里，我们可以采用梯度下降法来进行优化，用原来的值减去梯度乘以学习率的积，进行一次次迭代，最后得到最优解。或者说可以采用牛顿法，经过若干次迭代，最后也可以得到最优解，但是这样求解可能计算量有点大。

## 算法伪码
```
# 加载数据集
X_train, y_train, X_val, y_val, X_test, y_test = load_data()

loss_history = []

# 随机初始化W
W = np.random.randn((X_train.shape[0], X_train.shape[1]))
b = 0

# 进行训练
for i range(num_iters):
    loss, grads = train(X_train, y_train)  # 开始训练，计算损失值和梯度，采用梯度下降
    # 调整参数
    w -= s * grads["W"]   
    b -= s * grads["b"]
    
    pred = predict(X_val)  # 对验证集进行预测，对参数进行调整
    print("val_accuracy: %.4lf" % np.mean(pred, y_val))
    loss_history.append(loss)

# 输出在测试集中的精度
print("test_accuracy: %.4lf" % np.mean(predict(X_test), y_test))

```


